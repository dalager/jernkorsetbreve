{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import da_core_news_sm\n",
    "import re\n",
    "\n",
    "df_sentences = pd.read_csv('data/sentences.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text list from the worst\n",
    "# texts = worst.sentence.tolist()\n",
    "# tokens = nlp(texts[2])\n",
    "# tokens = [token.text for token in tokens]\n",
    "# #tokens\n",
    "# bert.predict(tokens, IOBformat=False)\n",
    "\n",
    "# extract named entities from the worst sentences\n",
    "# import spacy\n",
    "# import da_core_news_sm\n",
    "# nlp = da_core_news_sm.load()\n",
    "\n",
    "nlp = da_core_news_sm.load()\n",
    "\n",
    "def get_named_entities(text):\n",
    "    \"\"\"Extract named entities from text\"\"\"\n",
    "    # trim for whitespace, Bert does not like trailing whitespace. At all.\n",
    "    text = re.sub(r'\\x95', '', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    preds = bert.predict(tokens, IOBformat=False)\n",
    "    return [(ent['text'], ent['type']) for ent in preds['entities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector =[]# get named entities for each sentence\n",
    "bad_sentences=[]\n",
    "for index, row in df_sentences.iterrows():\n",
    "    text = row['sentence'].strip()\n",
    "    try:\n",
    "        ents = get_named_entities(text)\n",
    "        for ent in ents:\n",
    "            collector.append({'sentence_id': index, 'text': ent[0], 'type': ent[1]})\n",
    "    except Exception as e:\n",
    "        bad_sentences.append({'sentence_id': index, 'text': text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ents = pd.DataFrame(collector)\n",
    "df_ents.to_csv('NER_entities.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group and count per text and type\n",
    "grouped = df_ents.groupby(['text', 'type']).size().reset_index(name='counts').sort_values(by=['counts'], ascending=False)\n",
    "# export to csv\n",
    "\n",
    "# rename PER to person in values\n",
    "grouped['type'] = grouped['type'].replace('PER', 'person')\n",
    "# loc to sted\n",
    "grouped['type'] = grouped['type'].replace('LOC', 'sted')\n",
    "# org to organisation\n",
    "grouped['type'] = grouped['type'].replace('ORG', 'organisation')\n",
    "\n",
    "grouped.to_csv('NER_entities_grouped.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
