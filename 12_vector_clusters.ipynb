{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afsøgning af mønstre i breve med cluster analyse\n",
    "\n",
    "Cluster analyse med vektoriserede breve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dacy\n",
    "import plotly.express as px\n",
    "nlp = dacy.load('large')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_sentences = pd.read_csv('data/sentences.csv')\n",
    "df_letters = pd.read_csv('data/letters.csv')\n",
    "\n",
    "# Kun breve fra Peter\n",
    "df_letters = df_letters[df_letters['sender'] == 'Peter Mærsk']\n",
    "\n",
    "# Kun breve fra krigen, Torsdag 16. okt 1913 -> \n",
    "df_letters = df_letters[df_letters['date'] > '1913-10-15']\n",
    "# only sentences from df with letterIds from dfl\n",
    "df_sentences = df_sentences[df_sentences['letter_id'].isin(df_letters['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if not token.is_punct and not token.is_space]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letter vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Vi opdeler brevene i tokens (ca ord) og joiner dem igen for at kunne vectorisere.\n",
    "\n",
    "Desuden trunkerer vi dem til 512 ord pga begrænsninger i NLP modellen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only from Peter Mærsk\n",
    "def tokenize_letters(ldf):\n",
    "    print('Tokenizing letters... wait 32 min')\n",
    "    ldf['text_processed'] = ldf['text'].str.replace('<PARA>', ' ').astype(str)\n",
    "    ldf['text_processed'] = ldf['text_processed'].apply(lambda x: x.lower())\n",
    "\n",
    "    # truncate to 512 words\n",
    "    ldf['text_processed'] = ldf['text_processed'].str.split().str[:512].str.join(' ')\n",
    "    ldf['wordcount'] = ldf['text_processed'].str.split().str.len()\n",
    "\n",
    "    # det her tager ca 32 min\n",
    "    ldf['letter_tokenized'] = ldf['text_processed'].apply(tokenize)\n",
    "    ldf['letter_tokenized_joined'] = ldf['letter_tokenized'].apply(' '.join)\n",
    "    ldf.to_csv('data/letters_tokenized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if tokenized file exists, load it\n",
    "try:\n",
    "    df_letters = pd.read_csv('data/letters_tokenized.csv')\n",
    "except FileNotFoundError:\n",
    "    tokenize_letters(df_letters)\n",
    "    df_letters = pd.read_csv('data/letters_tokenized.csv')\n",
    "\n",
    "\n",
    "lettervectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False, preprocessor=None)\n",
    "lettervectors = lettervectorizer.fit_transform(df_letters['letter_tokenized_joined'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruger Elbow metoden til at finde det optimale antal clusters. Vælg tallet i \"albuen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sum_of_squared_distances = []\n",
    "K = range(1, 10)  # Adjust range as needed\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(lettervectors)\n",
    "    sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method For Optimal k - Letters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'vectors' is your TF-IDF matrix\n",
    "num_clusters = 3  # This is an arbitrary choice, you might need to experiment with this number\n",
    "\n",
    "# Create and fit the model\n",
    "letterkmeans = KMeans(n_clusters=num_clusters, n_init='auto', max_iter=1000, random_state=42)\n",
    "letterkmeans.fit(lettervectors)\n",
    "\n",
    "# Get cluster assignments for each document\n",
    "letterclusters = letterkmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'vectors' is your TF-IDF matrix and 'clusters' contains your cluster labels\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=2)  # Reducing to 2 dimensions for visualization\n",
    "reduced_vectors = pca.fit_transform(lettervectors.toarray())\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=letterclusters, cmap='viridis',s=10,alpha=0.5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.title('Cluster Visualization - Letters')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_plot = pd.DataFrame(reduced_vectors, columns=['PCA1', 'PCA2'])\n",
    "df_plot['Cluster'] = letterclusters\n",
    "df_plot['Text'] = df_letters['text'].str.replace('<PARA>', '\\n').astype(str)\n",
    "# Create the plot\n",
    "fig = px.scatter(df_plot, x='PCA1', y='PCA2', color='Cluster', title='Letter Cluster Visualization', hover_data=['Text'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization, Vectors and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the tokenized sentences don't exist\n",
    "def tokenize_sentences(df_sentences):\n",
    "    print('Tokenizing sentences... wait 45 min')\n",
    "    df_sentences['sentence_processed'] = df_sentences['sentence'].str.replace('<PARA>', ' ').astype(str)\n",
    "    df_sentences['sentence_processed'] = df_sentences['sentence_processed'].apply(lambda x: x.lower())\n",
    "    df_sentences['sentence_tokenized'] = df_sentences['sentence_processed'].apply(tokenize)\n",
    "    df_sentences['sentence_tokenized_joined'] = df_sentences['sentence_tokenized'].apply(' '.join)\n",
    "    df_sentences.to_csv('data/sentences_tokenized.csv', index=False)\n",
    "\n",
    "try:\n",
    "    df_sentences = pd.read_csv('data/sentences_tokenized.csv')\n",
    "except FileNotFoundError:\n",
    "    tokenize_sentences(df_sentences)\n",
    "    df_sentences = pd.read_csv('data/sentences_tokenized.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_sents = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False, preprocessor=None)\n",
    "vectors_sents = vectorizer_sents.fit_transform(df_sentences['sentence_tokenized_joined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sum_of_squared_distances = []\n",
    "K = range(1, 10)  # Adjust range as needed\n",
    "\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(vectors_sents)\n",
    "    sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method For Optimal k - Sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'vectors' is your TF-IDF matrix\n",
    "num_clusters = 4  # This is an arbitrary choice, you might need to experiment with this number\n",
    "\n",
    "# Create and fit the model\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init='auto', max_iter=1000, random_state=42)\n",
    "kmeans.fit(vectors_sents)\n",
    "\n",
    "# Get cluster assignments for each document\n",
    "sentence_clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'vectors' is your TF-IDF matrix and 'clusters' contains your cluster labels\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=2)  # Reducing to 2 dimensions for visualization\n",
    "reduced_sents_vectors = pca.fit_transform(vectors_sents.toarray())\n",
    "\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(reduced_sents_vectors[:, 0], reduced_sents_vectors[:, 1], c=sentence_clusters, s=10,alpha=0.5, cmap='viridis')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Sentence Clusters\")\n",
    "plt.title('Sentence Cluster Visualization')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 1000)\n",
    "#df[clusters == 2]['sentence_tokenized_joined'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df_plot = pd.DataFrame(reduced_sents_vectors, columns=['PCA1', 'PCA2'])\n",
    "df_plot['Cluster'] = sentence_clusters\n",
    "df_plot['Sentence'] = df_sentences['sentence']\n",
    "# Create the plot\n",
    "\n",
    "fig = px.scatter(df_plot, x='PCA1', y='PCA2', color='Cluster', title='Sentence Cluster Visualization (Plotly)', hover_data=['Sentence'], width=1000, height=1000, opacity=0.5)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Viridis4\n",
    "\n",
    "# Prepare data\n",
    "cluster_labels_str = [str(label) for label in sentence_clusters]\n",
    "source = ColumnDataSource(\n",
    "    data=dict(x=reduced_sents_vectors[:, 0], \n",
    "              y=reduced_sents_vectors[:, 1], \n",
    "              cluster=cluster_labels_str,\n",
    "              sentence=df_sentences['sentence']))\n",
    "\n",
    "\n",
    "# Create figure\n",
    "p = figure(title=\"Cluster Visualization with Bokeh\", tools=\"pan,wheel_zoom,box_zoom,reset\")\n",
    "df_plot['Cluster'] = sentence_clusters\n",
    "df_plot['Sentence'] = df_sentences['sentence']\n",
    "\n",
    "p.scatter(x='x', y='y', source=source, legend_field='cluster', fill_alpha=0.4, size=5, color=factor_cmap('cluster', palette=Viridis4, factors=sorted(set(cluster_labels_str))))\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Cluster\", \"@cluster\"), \n",
    "    (\"(x, y)\", \"($x, $y)\"),\n",
    "    (\"Sentence\", \"@sentence\")])\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jernkors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
